{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Document Segmentation and RAG\n"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook contains the steps and code to answer questions using the flan-ul2 Foundation Model from watsonx.ai and Langchain. The document is segment into meaningful chunks and multiple files are generated based on the subtitle of the data for this use case. The approach eliminates the need to mention the chunk size and chunk overlap. Intelligent Document Processing technique of Watson Discovery had been used to split the document and a query is used to retrieve the all documents. These documents are ingested to ChromaDB and rest of the query process remains same. This approach is expected increase the retrieval metrics as the chunks are more contextually appropriate.\n\n## Contents\nThis notebooks contains the following:\n1. Setup of required libraries and modules\n2. Data Loading, pay attention to multiple files being downloaded\n3. Accessing LLM from WML\n4. Answering the question using RAG approach"}, {"metadata": {}, "cell_type": "markdown", "source": "## Install the dependencies"}, {"metadata": {}, "cell_type": "markdown", "source": "Before starting this step, ensure that the Watson Machine Learning service is created and associated with this project. It might take few minutes to install all the dependencies."}, {"metadata": {}, "cell_type": "code", "source": "!pip install \"ibm-watson-machine-learning>=1.0.320\" \n!pip install \"pydantic>=1.10.0\" \n!pip install langchain \n!pip install huggingface\n!pip install huggingface-hub\n!pip install sentence-transformers\n!pip install chromadb\n!pip install wget", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: ibm-watson-machine-learning>=1.0.320 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (1.0.326)\nRequirement already satisfied: pandas<1.6.0,>=0.24.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (1.4.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (0.8.10)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (2023.7.22)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (1.26.16)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (2.12.0)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (0.3.3)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (23.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (6.0.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning>=1.0.320) (2.31.0)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.320) (2.12.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.320) (0.10.0)\nRequirement already satisfied: ibm-cos-sdk-core==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.320) (2.12.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk-core==2.12.0->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.320) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning>=1.0.320) (2023.3.post1)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning>=1.0.320) (1.23.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning>=1.0.320) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning>=1.0.320) (2.0.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from importlib-metadata->ibm-watson-machine-learning>=1.0.320) (3.8.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from lomond->ibm-watson-machine-learning>=1.0.320) (1.16.0)\nCollecting pydantic>=1.10.0\n  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting annotated-types>=0.4.0\n  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\nCollecting typing-extensions>=4.6.1\n  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\nCollecting pydantic-core==2.10.1\n  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: typing-extensions, annotated-types, pydantic-core, pydantic\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\nSuccessfully installed annotated-types-0.6.0 pydantic-2.4.2 pydantic-core-2.10.1 typing-extensions-4.8.0\nCollecting langchain\n  Downloading langchain-0.0.319-py3-none-any.whl (1.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (6.0)\nCollecting dataclasses-json<0.7,>=0.5.7\n  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (1.23.1)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (2.31.0)\nCollecting jsonpatch<2.0,>=1.33\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (2.4.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (1.4.39)\nCollecting langsmith<0.1.0,>=0.0.43\n  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain) (8.2.2)\nCollecting anyio<4.0\n  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: idna>=2.8 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.0.4)\nCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jsonpointer>=1.9\n  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.10.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nInstalling collected packages: typing-inspect, sniffio, marshmallow, jsonpointer, jsonpatch, dataclasses-json, anyio, langsmith, langchain\nSuccessfully installed anyio-3.7.1 dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.319 langsmith-0.0.49 marshmallow-3.20.1 sniffio-1.3.0 typing-inspect-0.9.0\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nCollecting huggingface-hub\n  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting filelock\n  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub) (4.8.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub) (23.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub) (6.0)\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub) (4.65.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.7.22)\nInstalling collected packages: fsspec, filelock, huggingface-hub\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2021.10.1\n    Uninstalling fsspec-2021.10.1:\n      Successfully uninstalled fsspec-2021.10.1\nSuccessfully installed filelock-3.12.4 fsspec-2023.9.2 huggingface-hub-0.18.0\nCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (1.12.1)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (0.13.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (1.23.1)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (1.1.1)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (1.8.1)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers) (0.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\nCollecting safetensors>=0.3.1\n  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting regex!=2019.12.17\n  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.15,>=0.14\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.0.1)\nCollecting huggingface-hub>=0.4.0\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\nBuilding wheels for collected packages: sentence-transformers\n", "name": "stdout"}, {"output_type": "stream", "text": "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125940 sha256=35b8f265b02087794bcd1a0cfcf2a4dab16b6285fb8559ebf8b6c0ba8f480982\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: safetensors, regex, nltk, huggingface-hub, tokenizers, transformers, sentence-transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.18.0\n    Uninstalling huggingface-hub-0.18.0:\n      Successfully uninstalled huggingface-hub-0.18.0\nSuccessfully installed huggingface-hub-0.17.3 nltk-3.8.1 regex-2023.10.3 safetensors-0.4.0 sentence-transformers-2.2.2 tokenizers-0.14.1 transformers-4.34.1\nCollecting chromadb\n  Downloading chromadb-0.4.14-py3-none-any.whl (448 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (0.14.1)\nCollecting posthog>=2.4.0\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nCollecting pypika>=0.48.9\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bcrypt>=4.0.1\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pulsar-client>=3.1.0\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.28 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (2.31.0)\nCollecting onnxruntime>=1.14.1\n  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting importlib-resources\n  Downloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\nCollecting grpcio>=1.58.0\n  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting fastapi>=0.95.2\n  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (4.65.0)\nCollecting typer>=0.9.0\n  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting overrides>=7.3.1\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (1.23.1)\nCollecting chroma-hnswlib==0.7.3\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (4.8.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb) (2.4.2)\nRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\nCollecting starlette<0.28.0,>=0.27.0\n  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting coloredlogs\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\nRequirement already satisfied: protobuf in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.19.6)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (2.0)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: pydantic-core==2.10.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.10.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\nRequirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.17.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.0.4)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httptools>=0.5.0\n  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\nCollecting python-dotenv>=0.13\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting watchfiles>=0.13\n  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n", "name": "stdout"}, {"output_type": "stream", "text": "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n  Downloading uvloop-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting websockets>=10.4\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.0.4)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.9.2)\nCollecting humanfriendly>=9.1\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53738 sha256=b04b1b62deff212aa9da53ce7a9caaaa2cb66e48b7fc4d8d41a16df39c2ba4f7\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, mpmath, monotonic, websockets, uvloop, typer, sympy, python-dotenv, pulsar-client, overrides, importlib-resources, humanfriendly, httptools, h11, grpcio, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.53.0\n    Uninstalling grpcio-1.53.0:\n      Successfully uninstalled grpcio-1.53.0\nSuccessfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.14 coloredlogs-15.0.1 fastapi-0.104.0 grpcio-1.59.0 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-resources-6.1.0 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.16.1 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 sympy-1.12 typer-0.9.0 uvicorn-0.23.2 uvloop-0.18.0 watchfiles-0.21.0 websockets-11.0.3\nCollecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=0c84dc552659e433cdfa436028b81d3ed891e13789c11e83874c14668b58387b\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## WatsonX.ai API Connection"}, {"metadata": {}, "cell_type": "markdown", "source": "Provide the Cloud IAM key to access the foundation models from the WML endpoint"}, {"metadata": {}, "cell_type": "code", "source": "import os, getpass\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}", "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your WML api key (hit enter): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Project Id definition"}, {"metadata": {}, "cell_type": "markdown", "source": "The foundation models need project id for the execution and also for the CUH"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\n   \nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The below cell downloads multiple files from git repo. These files have been split using Watson Discovery and a query was used to retrieve all the files and saved indiviudally.Only 4 files have been used to illustrate the approach."}, {"metadata": {}, "cell_type": "code", "source": "\nimport wget\n\nurl1 = 'https://raw.github.com/ravisrirangam/chunking_techniques/main/data/code_of_conduct.txt'\nwget.download(url1, out='code_of_conduct.txt')\nurl2 = 'https://raw.github.com/ravisrirangam/chunking_techniques/main/data/internet_email_policy.txt'\nwget.download(url2, out='internet_email_policy.txt')\nurl3 = 'https://raw.github.com/ravisrirangam/chunking_techniques/main/data/mobile_phone_policy.txt'\nwget.download(url3, out='mobile_phone_policy.txt')\nurl4 = 'https://raw.github.com/ravisrirangam/chunking_techniques/main/data/recruitment_policy.txt'\nwget.download(url4, out='recruitment_policy.txt')\nprint('files downloaded')", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "files downloaded\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Data Loading "}, {"metadata": {}, "cell_type": "markdown", "source": "The downloaded files are encoded using the default embedding model from HF and ingested to ChromaDB"}, {"metadata": {}, "cell_type": "code", "source": "from langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings()", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9393511a064148a7a66a2cde31ddb599"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7ab4f46c105745f29070bd8f6b6628bd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f353eeb5afe043f789c1d8ffedaedbe8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9b44b00326cf429caf58ffbbca4da613"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1a90f2336add4ba68ec47d0d08bb8343"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "99abb1ad238346dd9f5d3619fa168f77"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "03dae2d3ecb64ba69ccaac739c41eda7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "122acaabcc914d3a803041edcb174d8e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fc755178409a4d859fde50f82e0979df"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8fac7d17cf754ff6958ee5486cda95c6"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8d2d3e2ba65a496384838c649b4ebeb4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4bc0c7aad78d4d87a90c4df7658e1bf3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "77e2b7f8d2e7434ca816256773aa8f88"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3f0db7fef999423f942a51c50263c813"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import Chroma\n\nfilenames = ['code_of_conduct.txt', 'internet_email_policy.txt', 'mobile_phone_policy.txt', 'recruitment_policy.txt']\nfor i in range(len(filenames)):\n    filename = filenames[i]    \n    loader = TextLoader(filename)\n    document = loader.load()\n    Chroma.from_documents(document, embeddings)\n    print(filename, ' ingested')", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "code_of_conduct.txt  ingested\ninternet_email_policy.txt  ingested\nmobile_phone_policy.txt  ingested\nrecruitment_policy.txt  ingested\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "One more policy file is added, this step is done to get reference of the chromadb to be used for retrieval, this could be done in earlier step as well"}, {"metadata": {}, "cell_type": "code", "source": "url = 'https://raw.github.com/ravisrirangam/chunking_techniques/main/data/smoking_policy.txt'\nwget.download(url, out='smoking_policy.txt')\nprint('file downloaded')\nloader = TextLoader(filename)\ndocument = loader.load()\ndocsearch = Chroma.from_documents(document, embeddings)\nprint('Smoking policy ingested')", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "file downloaded\nSmoking policy ingested\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## flan-ul2 creation"}, {"metadata": {}, "cell_type": "markdown", "source": "The below code does the following:\n1. Get the model_id\n2. Create the parameters for the model\n3. Initialize the model\n4. Langchain wrapper for the model"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.FLAN_UL2", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The decoding method is set to \"greedy\" to get a deterministic output, you can change it to \"sample\" and add temperature, top_k and top_p parameters"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 130,\n    GenParams.MAX_NEW_TOKENS: 200\n}", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\n\nmodel = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id\n)", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n\nflan_ul2_llm = WatsonxLLM(model=model)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Get Answer to a question on one policy using RAG"}, {"metadata": {}, "cell_type": "markdown", "source": "The below code shows the retrieval part from ChromaDB. The query need not have the \"policy\" keyword. The quality of the retrieval can be enhanced by using all-mini* embedding model. The code is for illustrative purpose only. Using PromptTemplate and other advanced classes from Langchain the output can be improved."}, {"metadata": {}, "cell_type": "code", "source": "from langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(llm=flan_ul2_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\nquery = \"code of conduct\"\nqa.run(query)", "execution_count": 26, "outputs": [{"output_type": "execute_result", "execution_count": 26, "data": {"text/plain": "\"Yes, it outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability. Integrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest. Respect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy. Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters. Safety: We prioritize the safety of our employees, clients, and the communities we serve. We\""}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Query on ChromaDB"}, {"metadata": {}, "cell_type": "markdown", "source": "If a native query was run on ChromaDB with \"mobile policy\", it can be noticed that 4 chunks have been retrieved. The first two chunks are matching the query and are the correct chunks. The next two chunks are not relevant to the query but had some semantic match and were returned in the query result. The LLM picked up the correct chunks and generated the summary of the content."}, {"metadata": {}, "cell_type": "code", "source": "query = \"mobile policy\"\ndocs = docsearch.similarity_search(query)\nprint(len(docs))\nfor i in range(len(docs)):\n    print(docs[i].page_content)\n    print('\\n\\n')\n", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "4\nMobile Phone Policy\nThe Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance.\nAcceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\nSecurity: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\nConfidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.\nCost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.\nCompliance: Adhere to all pertinent laws and regulations concerning mobile phone usage, including those related to data protection and privacy.\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\nConsequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.\nThe Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n\n\n\n\nInternet and Email Policy\nOur Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and the importance of adhering to principles that maintain security, productivity, and legal compliance.\nAcceptable Use: Company-provided internet and email services are primarily meant for job-related tasks. Limited personal use is allowed during non-work hours, provided it doesn't interfere with work responsibilities.\nSecurity: Safeguard your login credentials, avoiding the sharing of passwords. Exercise caution with email attachments and links from unknown sources. Promptly report any unusual online activity or potential security breaches.\nConfidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing company matters on public forums or social media.\nHarassment and Inappropriate Content: Internet and email usage must not involve harassment, discrimination, or the distribution of offensive or inappropriate content. Show respect and sensitivity to others in all online communications.\nCompliance: Ensure compliance with all relevant laws and regulations regarding internet and email usage, including those related to copyright and data protection.\nMonitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\nConsequences: Policy violations may lead to disciplinary measures, including potential termination.\nOur Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and follow this policy. Regular reviews ensure its alignment with evolving technology and security standards.\n\n\n\n\nRecruitment Policy\nOur Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.\nEqual Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively promote diversity and inclusion.\nTransparency: We maintain transparency in our recruitment processes. All job vacancies are advertised internally and externally when appropriate. Job descriptions and requirements are clear and accurately represent the role.\nSelection Criteria: Our selection process is based on the qualifications, experience, and skills necessary for the position. Interviews and assessments are conducted objectively, and decisions are made without bias.\nData Privacy: We are committed to protecting the privacy of candidates' personal information and adhere to all relevant data protection laws and regulations.\nFeedback: Candidates will receive timely and constructive feedback on their application and interview performance.\nOnboarding: New employees receive comprehensive onboarding to help them integrate into the organization effectively. This includes information on our culture, policies, and expectations.\nEmployee Referrals: We encourage and appreciate employee referrals as they contribute to building a strong and engaged team.\nOur Recruitment Policy is a foundation for creating a diverse, inclusive, and talented workforce. It ensures that we attract and hire the best candidates who align with our company values and contribute to our continued success. We continuously review and update this policy to reflect evolving best practices in recruitment.\n\n\n\n\nRecruitment Policy\nOur Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.\nEqual Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively promote diversity and inclusion.\nTransparency: We maintain transparency in our recruitment processes. All job vacancies are advertised internally and externally when appropriate. Job descriptions and requirements are clear and accurately represent the role.\nSelection Criteria: Our selection process is based on the qualifications, experience, and skills necessary for the position. Interviews and assessments are conducted objectively, and decisions are made without bias.\nData Privacy: We are committed to protecting the privacy of candidates' personal information and adhere to all relevant data protection laws and regulations.\nFeedback: Candidates will receive timely and constructive feedback on their application and interview performance.\nOnboarding: New employees receive comprehensive onboarding to help them integrate into the organization effectively. This includes information on our culture, policies, and expectations.\nEmployee Referrals: We encourage and appreciate employee referrals as they contribute to building a strong and engaged team.\nOur Recruitment Policy is a foundation for creating a diverse, inclusive, and talented workforce. It ensures that we attract and hire the best candidates who align with our company values and contribute to our continued success. We continuously review and update this policy to reflect evolving best practices in recruitment.\n\n\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}